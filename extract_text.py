# -*- coding: utf-8 -*-import reimport osimport fnmatchimport math# Define a function in order to filter out parenthetical lines (undesired)def not_paren(element):  return element[0] != '(' and element[-1] != ')'# Define a function in order to filter out unspoken [and meaningless] linesdef unspoken(element):  if element.isupper():    return False  if re.search('\s\s\s', element):    return False  if "Converted to PDF" in element:    return False  if re.search('[a-zA-Z]', element):    return Truedef get_convos(path):  lines1 = []  convo_files = []  for root, child_dirs, files in os.walk(path):    for file_ in fnmatch.filter(files, '*.txt'):      convo_files.append(os.path.join(root, file_))  for file in convo_files:        # Loop through all files in the directory. Put their text in a list    lines = open(file).read().split('\n')        # Extract all entries except the first few (some have a variable length header) and the final (usually 'END')    #    (These are unspoken lines)    lines = lines[40:-2]      # Filter out all blank lines (represented as '' in the list)    lines = filter(None, lines)        # Filter out all parenthetical lines    lines = filter(not_paren, lines)    # Traverse the file and append its text to the list lines1    i = 0    while i < len(lines)-1:      # Concatenate lines part of the same utterance and only add spoken lines, not speaker information      if lines[i].isupper() or lines[i+1].isupper():        lines1.append(lines[i])        i += 1      else:        end = i+1        while end < len(lines):          if not lines[end].isupper():            end += 1          else:            break        item = " ".join(lines[i:end])        lines1.append(item)        i = end    # Filter out all unspoken [and meaningless] lines  lines1 = filter(unspoken, lines1)  numLines = len(lines1)  # Write results to an outside file, total.txt  writer = open(path + 'total.txt', 'a')  for line in lines1:    writer.write(line)    writer.write('\n')  writer.close()  return numLinesdef split_convos(path, numLines):  numLines -= 100  num_lines = numLines / 2			# Integer division so 1 line may be disregarded  test_size = int(math.floor(num_lines * 0.2))	# Again, some lines may be disregarded  print("num lines: %d" % num_lines)  print("train size: %d" % test_size)  # Open the file that contains Text to be used  lines = open(path + '/total.txt').read().split('\n')  # Slice the file's contents into two categories (somewhat like 'queries' and 'responses')  firstLines = lines[100::2]  secondLines = lines[100::2]  # Open the files to be written to  train_enc = open(path + 'train.enc','w')  train_dec = open(path + 'train.dec','w')  test_enc = open(path + 'test.enc','w')  test_dec = open(path + 'test.dec','w')    for i in range(num_lines):    # This part splits in a deterministic manner. If we only train once,     #   it shouldn't matter (this isn't a commonly used dataset anyway).    #    one could consider randomly setting i, though    if i < test_size:      # Write 80% of the lines to train files      test_enc.write(firstLines[i]+'\n')      test_dec.write(secondLines[i+1]+'\n')    else:      # Write 80% of the lines to test files      train_enc.write(firstLines[i]+'\n')      train_dec.write(secondLines[i+1]+'\n')    # Close the files as we are now done  train_enc.close()  train_dec.close()  test_enc.close()  test_dec.close()if __name__ == '__main__':  path = '/home/jjeng/Downloads/dialogs/'  print('Reading text files...')  numLines = get_convos(path)  print('Read all text files!')  print('Splitting to train and test sets...')  split_convos(path, numLines)  print('Done splitting to train and test sets!')
